{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_NLP@SAMSUNG.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNhcmVgFfgPaaUNbQoBsB0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julichitai/learning_NN/blob/NLP/text_classification_NLP_SAMSUNG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaqG73vNuDgh"
      },
      "source": [
        "## Тематическая классификация длинных текстов - TFIDF и LogReg¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD_VxM7gt450",
        "outputId": "847e7688-a6d0-4127-e3a8-79a69027873b"
      },
      "source": [
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 289 (delta 10), reused 14 (delta 6), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (289/289), 42.27 MiB | 23.02 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.22.2.post1)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.7.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.41.1)\n",
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 20.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.11.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading https://files.pythonhosted.org/packages/39/6f/86bd5d0eaa6821ba9193bbed16b660ea6f342fe63ec2e4fa2c61377bb44b/pyconll-2.3.3-py3-none-any.whl\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 141kB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Collecting livelossplot==0.5.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/08/1884157a3de72d41fa97cacacafaa49abf00eba53cb7e08615b2b65b4a9d/livelossplot-0.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.0.0)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.2.4)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 25.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.1)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (51.0.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: requests>=2.21 in /usr/local/lib/python3.6/dist-packages (from pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (4.0.1)\n",
            "Requirement already satisfied: bokeh; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.4.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.7.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (20.0.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (20.8)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (7.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.11.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.13)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.4.0)\n",
            "Building wheels for collected packages: wget, ufal.udpipe, intervaltree\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=4215f0eb98ec77e7f996831dcbcb97330df4feee5d2fb9f1dc5b1e11d180a979\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625304 sha256=ed044bc9ac6ea80d66f62f96802f3aa8c168ccc4d97b34b9fd37c7c14ee22eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26102 sha256=4839a9212f9276b092d2ac58f4978fc57e329ad66eaedebcd198f765fcffcd5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
            "Successfully built wget ufal.udpipe intervaltree\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe, dawg-python, pymorphy2-dicts-ru, pymorphy2, intervaltree, ipymarkup, youtokentome, pyconll, gensim, wget, livelossplot\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP3sA5lxsKDB"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import dlnlputils\n",
        "# from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, vectorize_texts, SparseFeaturesDataset, TOKEN_RE\n",
        "# from dlnlputils.pipeline import train_eval_loop, init_random_seed, predict_with_model\n",
        "\n",
        "from dlnlputils.pipeline import init_random_seed\n",
        "from dlnlputils.data import TOKEN_RE\n",
        "\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILAtvxItxMHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f404396-6eba-44f4-b1af-566c0c707186"
      },
      "source": [
        "TOKEN_RE"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "re.compile(r'[\\w\\d]+', re.UNICODE)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12zAUJphwDHa"
      },
      "source": [
        "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
        "    txt = txt.lower()\n",
        "    all_tokens = TOKEN_RE.findall(txt)\n",
        "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
        "\n",
        "\n",
        "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
        "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
        "\n",
        "\n",
        "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
        "    word_counts = collections.defaultdict(int)\n",
        "    doc_n = 0\n",
        "\n",
        "    # посчитать количество документов, в которых употребляется каждое слово\n",
        "    # а также общее количество документов\n",
        "    for txt in tokenized_texts:\n",
        "        doc_n += 1\n",
        "        unique_text_tokens = set(txt)\n",
        "        for token in unique_text_tokens:\n",
        "            word_counts[token] += 1\n",
        "\n",
        "    # убрать слишком редкие и слишком частые слова\n",
        "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
        "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
        "\n",
        "    # отсортировать слова по убыванию частоты\n",
        "    sorted_word_counts = sorted(word_counts.items(),\n",
        "                                reverse=True,\n",
        "                                key=lambda pair: pair[1])\n",
        "\n",
        "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
        "    if pad_word is not None:\n",
        "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
        "\n",
        "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
        "    if len(word_counts) > max_size:\n",
        "        sorted_word_counts = sorted_word_counts[:max_size]\n",
        "\n",
        "    # нумеруем слова\n",
        "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
        "\n",
        "    # нормируем частоты слов\n",
        "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
        "\n",
        "    return word2id, word2freq\n",
        "\n",
        "\n",
        "def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n",
        "    assert mode in {'tfidf', 'idf', 'tf', 'bin'}\n",
        "\n",
        "    # считаем количество употреблений каждого слова в каждом документе\n",
        "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                result[text_i, word2id[token]] += 1\n",
        "\n",
        "    # получаем бинарные вектора \"встречается или нет\"\n",
        "    if mode == 'bin':\n",
        "        result = (result > 0).astype('float32')\n",
        "\n",
        "    # получаем вектора относительных частот слова в документе\n",
        "    elif mode == 'tf':\n",
        "        result = result.tocsr()\n",
        "        result = result.multiply(1 / result.sum(1))\n",
        "\n",
        "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
        "    # но оставляем информацию о частотности слова в корпусе в целом\n",
        "    elif mode == 'idf':\n",
        "        result = (result > 0).astype('float32').multiply(1 / word2freq)\n",
        "\n",
        "    # учитываем всю информацию, которая у нас есть:\n",
        "    # частоту слова в документе и частоту слова в корпусе\n",
        "    elif mode == 'tfidf':\n",
        "        result = result.tocsr()\n",
        "        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n",
        "        result = result.multiply(1 / word2freq)  # разделить каждый столбец на вес слова\n",
        "\n",
        "    if scale:\n",
        "        result = result.tocsc()\n",
        "        result -= result.min()\n",
        "        result /= (result.max() + 1e-6)\n",
        "\n",
        "    return result.tocsr()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMpJg6OnwSyb"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SparseFeaturesDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
        "        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
        "        return cur_features, cur_label"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7JllmaFwoEY"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
        "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
        "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
        "                    max_batches_per_epoch_train=10000,\n",
        "                    max_batches_per_epoch_val=1000,\n",
        "                    data_loader_ctor=DataLoader,\n",
        "                    optimizer_ctor=None,\n",
        "                    lr_scheduler_ctor=None,\n",
        "                    shuffle_train=True,\n",
        "                    dataloader_workers_n=0):\n",
        "    \"\"\"\n",
        "    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n",
        "    :param model: torch.nn.Module - обучаемая модель\n",
        "    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n",
        "    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n",
        "    :param criterion: функция потерь для настройки модели\n",
        "    :param lr: скорость обучения\n",
        "    :param epoch_n: максимальное количество эпох\n",
        "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
        "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
        "    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
        "        отсутствие улучшения модели, чтобы обучение продолжалось.\n",
        "    :param l2_reg_alpha: коэффициент L2-регуляризации\n",
        "    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n",
        "    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n",
        "    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n",
        "        (по умолчанию torch.utils.data.DataLoader)\n",
        "    :return: кортеж из двух элементов:\n",
        "        - среднее значение функции потерь на валидации на лучшей эпохе\n",
        "        - лучшая модель\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "\n",
        "    if optimizer_ctor is None:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
        "    else:\n",
        "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
        "\n",
        "    if lr_scheduler_ctor is not None:\n",
        "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
        "    else:\n",
        "        lr_scheduler = None\n",
        "\n",
        "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                                        num_workers=dataloader_workers_n)\n",
        "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                      num_workers=dataloader_workers_n)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch_i = 0\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    for epoch_i in range(epoch_n):\n",
        "        try:\n",
        "            epoch_start = datetime.datetime.now()\n",
        "            print('Эпоха {}'.format(epoch_i))\n",
        "\n",
        "            model.train()\n",
        "            mean_train_loss = 0\n",
        "            train_batches_n = 0\n",
        "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "                if batch_i > max_batches_per_epoch_train:\n",
        "                    break\n",
        "\n",
        "                batch_x = copy_data_to_device(batch_x, device)\n",
        "                batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                pred = model(batch_x)\n",
        "                loss = criterion(pred, batch_y)\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                mean_train_loss += float(loss)\n",
        "                train_batches_n += 1\n",
        "\n",
        "            mean_train_loss /= train_batches_n\n",
        "            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n",
        "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
        "            print('Среднее значение функции потерь на обучении', mean_train_loss)\n",
        "\n",
        "\n",
        "\n",
        "            model.eval()\n",
        "            mean_val_loss = 0\n",
        "            val_batches_n = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
        "                    if batch_i > max_batches_per_epoch_val:\n",
        "                        break\n",
        "\n",
        "                    batch_x = copy_data_to_device(batch_x, device)\n",
        "                    batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                    pred = model(batch_x)\n",
        "                    loss = criterion(pred, batch_y)\n",
        "\n",
        "                    mean_val_loss += float(loss)\n",
        "                    val_batches_n += 1\n",
        "\n",
        "            mean_val_loss /= val_batches_n\n",
        "            print('Среднее значение функции потерь на валидации', mean_val_loss)\n",
        "\n",
        "            if mean_val_loss < best_val_loss:\n",
        "                best_epoch_i = epoch_i\n",
        "                best_val_loss = mean_val_loss\n",
        "                best_model = copy.deepcopy(model)\n",
        "                print('Новая лучшая модель!')\n",
        "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
        "                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n",
        "                    early_stopping_patience))\n",
        "                break\n",
        "\n",
        "            if lr_scheduler is not None:\n",
        "                lr_scheduler.step(mean_val_loss)\n",
        "\n",
        "            print()\n",
        "        except KeyboardInterrupt:\n",
        "            print('Досрочно остановлено пользователем')\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n",
        "            break\n",
        "\n",
        "    return best_val_loss, best_model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPgG_jivwtY1"
      },
      "source": [
        "def predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n",
        "    \"\"\"\n",
        "    :param model: torch.nn.Module - обученная модель\n",
        "    :param dataset: torch.utils.data.Dataset - данные для применения модели\n",
        "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
        "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
        "    :return: numpy.array размерности len(dataset) x *\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    results_by_batch = []\n",
        "\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        import tqdm\n",
        "        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n",
        "            batch_x = copy_data_to_device(batch_x, device)\n",
        "\n",
        "            if return_labels:\n",
        "                labels.append(batch_y.numpy())\n",
        "\n",
        "            batch_pred = model(batch_x)\n",
        "            results_by_batch.append(batch_pred.detach().cpu().numpy())\n",
        "\n",
        "    if return_labels:\n",
        "        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n",
        "    else:\n",
        "        return np.concatenate(results_by_batch, 0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-S-_aJZuHR7"
      },
      "source": [
        "### Предобработка текстов и подготовка признаков\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1h7e4D7sOp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6700d36c-c467-404f-b909-ca771a4f150a"
      },
      "source": [
        "train_data = fetch_20newsgroups(subset='train')\n",
        "test_data = fetch_20newsgroups(subset='test')\n",
        "\n",
        "print('Количество обучающих текстов', len(train_data['data']))\n",
        "print('Количество тестовых текстов', len(test_data['data']))\n",
        "\n",
        "print()\n",
        "print(train_data['data'][0].strip())\n",
        "print()\n",
        "print('Имена целевых переменных', train_data['target_names'])\n",
        "print('Метка', train_data['target'][0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество обучающих текстов 11314\n",
            "Количество тестовых текстов 7532\n",
            "\n",
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "Имена целевых переменных ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "Метка 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWjg6HIMuJxf"
      },
      "source": [
        "#### Подготовка признаков\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwl2A9tcw8IB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87dee975-d3f6-4b19-d268-76100de104f0"
      },
      "source": [
        "train_tokenized = tokenize_corpus(train_data['data'])\n",
        "test_tokenized = tokenize_corpus(test_data['data'])\n",
        "\n",
        "print(' '.join(train_tokenized[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from lerxst where thing subject what this nntp posting host rac3 organization university maryland college park lines wondering anyone there could enlighten this other door sports looked from late early called bricklin doors were really small addition front bumper separate from rest body this know anyone tellme model name engine specs years production where this made history whatever info have this funky looking please mail thanks brought your neighborhood lerxst\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYZ7KEchuM3n"
      },
      "source": [
        "#### Распределение классов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eQy-NpEuNHl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGOYhRo_uNTf"
      },
      "source": [
        "#### PyTorch Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HteNBe57uR9A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcV40rkuuTOB"
      },
      "source": [
        "### Обучение модели на PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIAEwwbDuUaf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NhgovcUuVbH"
      },
      "source": [
        "### Оценка качества\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx189fSkuWY5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xesMHwu9uXfT"
      },
      "source": [
        "## Альтернативная реализация на scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxnLcP5uYdO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvC2Ft-YuZ-h"
      },
      "source": [
        "### Оценка качества\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CRgEL0FuamX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}